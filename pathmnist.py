# -*- coding: utf-8 -*-
"""pathmnist.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XrzWIhC83DfWc4IPXSUMAPGnmrPEvG3h
"""

# Cell 1: Install and import required libraries
!pip install medmnist
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torchvision import transforms
import medmnist
from medmnist import INFO
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix
import seaborn as sns
from PIL import Image

# Set device
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")

# Cell 2: Load and prepare the dataset
data_flag = 'pathmnist'  # Using PathMNIST as example for tumor detection
info = INFO[data_flag]
n_classes = len(info['label'])
DataClass = getattr(medmnist, info['python_class'])

# Data transformations
train_transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.RandomRotation(15),
    transforms.RandomHorizontalFlip(),
    transforms.Normalize(mean=[0.5], std=[0.5])
])

test_transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5])
])

# Load datasets
train_dataset = DataClass(split='train', transform=train_transform, download=True)
val_dataset = DataClass(split='val', transform=test_transform, download=True)
test_dataset = DataClass(split='test', transform=test_transform, download=True)

# Create data loaders
batch_size = 128
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

print(f"Train samples: {len(train_dataset)}, Val samples: {len(val_dataset)}, Test samples: {len(test_dataset)}")
print(f"Number of classes: {n_classes}")

# Cell 3: Define the model architecture
class BrainTumorClassifier(nn.Module):
    def __init__(self, num_classes=n_classes):
        super(BrainTumorClassifier, self).__init__()
        self.features = nn.Sequential(
            nn.Conv2d(3, 32, kernel_size=3, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU(),
            nn.MaxPool2d(2, 2),
            nn.Conv2d(32, 64, kernel_size=3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(),
            nn.MaxPool2d(2, 2),
            nn.Conv2d(64, 128, kernel_size=3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(),
            nn.MaxPool2d(2, 2)
        )
        self.classifier = nn.Sequential(
            nn.Linear(128 * 3 * 3, 256),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(256, num_classes)
        )

    def forward(self, x):
        x = self.features(x)
        x = torch.flatten(x, 1)
        x = self.classifier(x)
        return x

model = BrainTumorClassifier().to(device)
print(model)

# Cell 4: Define training and evaluation functions
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)
scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3)

def evaluate_model(model, data_loader, criterion):
    model.eval()
    running_loss, running_correct, total = 0.0, 0, 0

    with torch.no_grad():
        for inputs, targets in data_loader:
            inputs, targets = inputs.to(device), targets.squeeze().long().to(device)
            outputs = model(inputs)
            loss = criterion(outputs, targets)
            running_loss += loss.item()
            _, preds = torch.max(outputs, 1)
            running_correct += (preds == targets).sum().item()
            total += targets.size(0)

    return running_loss / len(data_loader), running_correct / total

def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=25):
    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}

    for epoch in range(num_epochs):
        model.train()
        running_loss, running_correct, total = 0.0, 0, 0

        for inputs, targets in train_loader:
            inputs, targets = inputs.to(device), targets.squeeze().long().to(device)

            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, targets)
            loss.backward()
            optimizer.step()

            running_loss += loss.item()
            _, preds = torch.max(outputs, 1)
            running_correct += (preds == targets).sum().item()
            total += targets.size(0)

        train_loss = running_loss / len(train_loader)
        train_acc = running_correct / total

        val_loss, val_acc = evaluate_model(model, val_loader, criterion)
        scheduler.step(val_loss)

        history['train_loss'].append(train_loss)
        history['train_acc'].append(train_acc)
        history['val_loss'].append(val_loss)
        history['val_acc'].append(val_acc)

        print(f'Epoch {epoch+1}/{num_epochs}: '
              f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, '
              f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')

    return history

# Cell 5: Train the model
num_epochs = 3
print("Starting training...")
history = train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs)
print("Training completed!")

# Cell 6: Plot training history
plt.figure(figsize=(12, 5))

# Loss plot
plt.subplot(1, 2, 1)
plt.plot(history['train_loss'], label='Train Loss')
plt.plot(history['val_loss'], label='Validation Loss')
plt.title('Training and Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

# Accuracy plot
plt.subplot(1, 2, 2)
plt.plot(history['train_acc'], label='Train Accuracy')
plt.plot(history['val_acc'], label='Validation Accuracy')
plt.title('Training and Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

plt.tight_layout()
plt.show()

# Cell 7: Confusion Matrix
def plot_confusion_matrix(model, data_loader):
    model.eval()
    all_preds = []
    all_targets = []
    class_names = ['Normal', 'Tumor']  # Simplified for binary classification

    with torch.no_grad():
        for inputs, targets in data_loader:
            inputs, targets = inputs.to(device), targets.squeeze().long().to(device)
            outputs = model(inputs)
            _, preds = torch.max(outputs, 1)
            all_preds.extend(preds.cpu().numpy())
            all_targets.extend(targets.cpu().numpy())

    cm = confusion_matrix(all_targets, all_preds)
    plt.figure(figsize=(6, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)
    plt.title('Confusion Matrix for Brain Tumor Detection')
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.show()

print("Confusion Matrix on Test Set:")
plot_confusion_matrix(model, test_loader)

# Cell 8: Prediction and visualization function
def predict_and_visualize(model, dataset, index=0):
    model.eval()
    image, label = dataset[index]
    image = image.to(device).unsqueeze(0)

    with torch.no_grad():
        output = model(image)
        _, pred = torch.max(output, 1)
        probs = torch.softmax(output, dim=1)[0] * 100

    # Convert image for display
    img = image.squeeze().permute(1, 2, 0).cpu().numpy()
    img = (img - img.min()) / (img.max() - img.min())

    # Use the actual class names from the dataset info
    # Ensure 'info' is accessible in this scope or pass it as an argument
    class_names = list(info['label'].values())

    plt.figure(figsize=(8, 6))
    plt.imshow(img)
    plt.title(f'Actual: {class_names[label.item()]}\n'
              f'Predicted: {class_names[pred.item()]}\n'
              f'Confidence: {probs[pred.item()]:.2f}%')
    plt.axis('off')
    plt.show()

    return pred.item()

# Example prediction
sample_idx = 42  # Change this to test different images
print(f"Making prediction on sample index {sample_idx}:")
prediction = predict_and_visualize(model, test_dataset, sample_idx)
# The interpretation of the prediction depends on which class index corresponds to 'Tumor'
# Based on the original code's intent, if class index 1 is 'Tumor' in PathMNIST, this line is fine.
# Otherwise, you'd need to map the prediction index to the desired interpretation.
# For PathMNIST, the labels represent different tissue types. You might need to decide
# which class indices you consider 'Tumor' or adapt this line accordingly.
# For now, let's keep the original structure but acknowledge it's a multi-class prediction.
print(f"Final Prediction Class Index: {prediction}") # Changed to show class index

